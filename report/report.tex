% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{***} % *** Enter the CVPR Paper ID here
\def\confName{ x}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Visual Geo-Localization}

\author{Matteo Gambino\\
s287572
\and
Michele Pierro\\
s287846
\and
Fabio Grillo\\
s287873
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   In order to predict the location of a query image by retrieving annotated photographs with 
   similar descriptors needs an efficient and reliable generation of those descriptors. 
   In order to accomplish that objective, is fundamental that the network focuses on portion
   of the various images that contains useful information and at the same time ignore not 
   informative areas like the ones containing elements like cars or pedestrians. For that 
   reason attention layers are fundamental in the proposed network. In addition to that we 
   are comparing state of the art techniques for the visual geolocalization task like GeM \cite{GEM}, 
   NetVLAD \cite{NETVLAD} and CRN \cite{CRN}. The code used is publicly available 
   \href{https://github.com/matteo6198/project_visual_geolocalization}{here}
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

\section{Related works}

\section{Methods}
Like \cite{GEM}, \cite{NETVLAD}, \cite{CRN} we have casted the problem of place recognition as the task of image
retrieval. We have implemented 3 different networks all based on the ResNet-18 \cite{resnet} backbone
without the fully connected layers and the last convolution layer. On the top of this backbone we 
have inserted 4 different heads, inspired by the works of \cite{GEM}, \cite{NETVLAD}, \cite{CRN}, in order
to generate the image descriptors. 

\subsection{Base Head}
This is the simplest head we have used and it's necessary in order to have a baseline to compare the other
results. After the last convolution layer of ResNet-18 we have normalized the feature map and used average 
pooling in order to generate the descriptors. This simple head tries to extract from the query the spatial
information by comparing the average value of the features in a given area and represent the traditional way 
to extract those descriptors.

\subsection{GeM head}
Following the work of \cite{GEM}, we have used a Generalized Mean approach in order to extract better 
descriptors for the query image. The generalized mean we are using is defined as:
\begin{equation}
   f_k = ({{1}\over{X_k}} \sum_{x \in X_k} x^{p_k} ) ^ {1\over{p_k}}
\end{equation}
where $X_k $ represent one of the normalized features map and $p_k $ is the pooling parameter. This 
pooling parameter is expressing how much is localized the zone of the image the network is focusing on.
The $p_k $ parameter, although it can be learned and inserted into back propagation, it has been fixed
and a single value is used for each activation map as suggested by \cite{GEM}. We have
inserted a fully connected layer that takes as input the pooled features in order to whiten the image
descriptors since it has been shown by \cite{GEM} that this approach is providing better results than
using other strategies like PCA.

\subsection{NetVLAD head}

\subsection{CRN head}
Seen the results provided from the previous implemented heads and the success of the attention layers to make 
a network focus on relevant only parts of an image, we have decided to add an attention layer at the NetVLAD
head following the approach proposed by \cite{CRN}. This is perfectly integrated in the NetVLAD architecture and
it has the duty to produce a map that rescales the weights produced by the soft assignment step of the NetVLAD layer.
This layer is composed by an initial average pooling sub-layer
that has the duty to reduce the dimensionality of the feature maps produced by the backbone. 
Differently from what specified in \cite{CRN}, it is not reducing the features maps to a fixed 
size but it is simply reducing by a half the dimensions of the features. In order to capture features at different
spatial resolutions, $3$ convolution filters (with kernel sizes respectively of $3$, $5$, $7$) are applied to the pooled features.
The  output of those filter is concatenated and an additional $1x1$ convolution filter is used in order to accumulate the 
features produced. The resulting mask is then upsampled, in order to restore the original features map dimensionality,
by using a bilinear interpolator.This results into a mask that is used as to re-weight the features produced by the soft assignment specified
into the NetVLAD description as showed in figure \ref{fig:CRN:ark} into the context modulation layer. This layer is 
performing the product of the mask and the soft-assignment scores. The output of the context modulation layer is then used 
in the standard NetVLAD core instead of the soft-assignment scores. Also this head requires the initialization of centroids as the 
NetVLAD head.

\begin{figure}
   \centering
   \includegraphics[width=0.3\textwidth]{img/CRN.png}
   \caption{The architecture of the CRN head.}
   \label{fig:CRN:ark}
\end{figure}

\subsection{CRN2 head}
In order to reduce the number of parameters to be learned by the CRN head, we have implemented a second version of this head,
called CRN2, that exploits the ideas of concatenating multiple $3x3$ filters in order to obtain the same receptive field of a
bigger filter but using less parameters as suggested in \cite{VGG}. In particular we have replaced the $5x5$ and the $7x7$ filters showed in figure \ref{fig:CRN:ark}
with respectively 2 and 3 stacked $3x3$ filters as showed in figure \ref{fig:CRN2:ark}. In addition to that, we have used dilated convolution
in order to remove the pooling and upsampling layers by generating the mask directly at the desired resolution inspired by the work 
proposed in \cite{atrous}. This, although requires 
more computation, will produce more accurate masks that may help the network to better focus on the relevant parts of the images.
Also this head requires the initialization of centroids as the CRN and the NetVLAD heads.

\begin{figure}
   \centering
   \includegraphics[width=0.3\textwidth]{img/CRN2.png}
   \caption{The architecture of the CRN2 head.}
   \label{fig:CRN2:ark}
\end{figure}

\section{Experiments}
In this section we evaluate the results obtained by the implemented heads and the selection of best hyper-parameters will be discussed.

\begin{figure}
   \centering
   \begin{subfigure}[b]{0.23\textwidth}
      \centering
      \includegraphics[width=\textwidth]{img/pitts_out.png}
      \caption{Pitts30k}
      \label{fig:datasets:pitts30k}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{0.23\textwidth}
      \centering
      \includegraphics[width=\textwidth]{img/st_lucia_out.png}
      \begin{minipage}{1cm}
         \vfill
      \end{minipage}
      \vspace{0.2cm}
      \caption{St. Lucia}
      \label{fig:datasets:st_lucia}
   \end{subfigure}
   \caption{The location of the images contained in both the pitts30k \ref{fig:datasets:pitts30k} dataset and the St. Lucia \ref{fig:datasets:st_lucia} dataset.}
   \label{fig:datasets}
\end{figure}

\paragraph{Datasets}
The experiments have been run on the pitts30k dataset \cite{NETVLAD} that is containing 3 different predefined splits, respectively for 
training, validation and testing, composed by $10k$ images each. Those are images taken in the city of Pittsburgh from the Google street
view images. In addition, also the St. Lucia dataset \cite{st_lucia} has been used only for testing the models trained on the pitts30k dataset.
The location of the images contained in both datasets can be seen in figure \ref{fig:datasets}.

\paragraph{Mining procedure}
In order to train our models we need to generate triplets in the form $\{I_q, I^+, I^-\}$: for each query image $I_q$ we are looking for
positive examples $I^+$ and negative ones ($I^-$). In order to do so, for each query image we retrieve the images into the database that are
in a range specified by the positive distance threshold. We use 2 possibly different values for this threshold at train and test time.
Among all images in this range, we select as best positive the one that has descriptors more similar to the query image and we use this image
as $I^+$. All the images not in the positive distance threshold range are considered as negative samples. We select by default the 10 
images, from the negative set, that have the as similar as possible descriptors as the query image. We call those images hard negatives and
we are using those instead of randomly picking from the negative set in order to make the task for the network more challenging resulting in 
a more robust model. Since we have to generate the descriptors that vary over the training procedure, we are periodically recalculating the 
triplets within the epochs.

\paragraph{Metric adopted}
Those results have been obtained by evaluating the models by using a standard evaluation procedure for place recognition.
A given query image is said to be correctly localized if at least one of the $N$ retrieved images is placed at a distance lower
or equal to $TTD$ from the ground truth position. This distance is set, if not differently specified, to $25$ meters. After
that we are calculating the percentage of correctly classified images for different values of $N$ (indicated with $R@N$).  

\subsection{Comparison among the proposed heads}
The results of comparison between the various proposed heads are reported in table \ref{tab:base_results}. Those results have 
been obtained on the pitts30k test set. As it's possible to notice the head that is giving best results is the CRN and that 
shows that adding attention is essential for improving the quality of generated descriptors. It's important to notice also how 
the results are influenced by the number of produced descriptors. In fact both the NetVLAD and the CRN head are generating much 
more descriptors with respect to the GeM and the base heads and this seems correlated to higher recalls. Since the CRN and the NetVLAD
heads are outperforming the other ones, we will focus more on those 2 during the rest of this section.

\begin{table}
   \centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
         & Descs.&        $R@1$   &        $R@5$   &        $R@10$  &        $R@20$   \\ \hline
Base     & 256   &         60.1   &         80.6   &          87.4   &          91.7   \\
GeM      & 256   &         71.6   &         87.0   &          91.0   &          94.0   \\
NetVLAD  & 16384 &         79.1   &         89.3   &          92.3   &          94.4   \\ \hline
CRN*     & 16384 &         81.7   & \textbf{90.7}  &  \textbf{93.4}  &  \textbf{95.3}  \\
CRN2     & 16384 &\textbf{81.8}   & \textbf{90.7}  &          93.2   &          95.2   \\ \hline
\end{tabular}
\caption{Results on the pitts30k test set obtained with the various heads compared with the base head. The number of generated descriptors 
is also shown in the column Descs.}
\label{tab:base_results}
\end{table}

\begin{table}
   \centering
 \begin{tabular}{|l|c|c|c|c|}
\hline
          &          $R@1$  &        $R@5$  &        $R@10$ &        $R@20$   \\ \hline     
lr = 1e-3 &         81.9    &         91.8  &         92.5  &         94.7       \\
lr = 1e-4 &         82.2    &         93.0  & \textbf{95.4} & \textbf{97.1}  \\    
lr = 1e-5 & \textbf{83.5}   & \textbf{93.1} &         94.3  & \textbf{97.1}          \\
\hline
\end{tabular}
\caption{Results obtained with the NetVLAD head on the pitts30k validation set with different learning rates}
\label{tab:NETVLAD:lr}
\end{table}

\section{Ablation study}

\begin{figure}
   \centering
   \begin{subfigure}[b]{0.23\textwidth}
      \centering
      \includegraphics[width=\textwidth]{img/augment/test_pitts30k_recalls_graph.png}
      \caption{Pitts30k}
      \label{fig:recalls:augment:pitts30k}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{0.23\textwidth}
      \centering
      \includegraphics[width=\textwidth]{img/augment/test_st_lucia_recalls_graph.png}
      \caption{St. Lucia}
      \label{fig:recalls:augment:st_lucia}
   \end{subfigure}
   \caption{Graph showing the recalls obtained with different augmentation techniques on both the pitts30k \ref{fig:recalls:augment:pitts30k} dataset and the St. Lucia \ref{fig:recalls:augment:st_lucia} dataset.}
   \label{fig:recalls:augment}
\end{figure}

\begin{figure}
   \centering
   \begin{subfigure}[b]{0.23\textwidth}
      \centering
      \includegraphics[width=\textwidth]{img/resize/test_pitts30k_recalls_graph.png}
      \caption{Pitts30k}
      \label{fig:recalls:resize:pitts30k}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{0.23\textwidth}
      \centering
      \includegraphics[width=\textwidth]{img/resize/test_st_lucia_recalls_graph.png}
      \caption{St. Lucia}
      \label{fig:recalls:resize:st_lucia}
   \end{subfigure}
   \caption{Graph showing the recalls obtained with different input image size on both the pitts30k \ref{fig:recalls:resize:pitts30k} dataset and the St. Lucia \ref{fig:recalls:resize:st_lucia} dataset.}
   \label{fig:recalls:resize}
\end{figure}

\begin{figure}
   \centering
   \begin{subfigure}[b]{0.23\textwidth}
      \centering
      \includegraphics[width=\textwidth]{img/train_th/test_pitts30k_recalls_graph.png}
      \caption{Pitts30k}
      \label{fig:recalls:train_th:pitts30k}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{0.23\textwidth}
      \centering
      \includegraphics[width=\textwidth]{img/train_th/test_st_lucia_recalls_graph.png}
      \caption{St. Lucia}
      \label{fig:recalls:train_th:st_lucia}
   \end{subfigure}
   \caption{Graph showing the recalls obtained with different train positive distance threshold on both the pitts30k \ref{fig:recalls:train_th:pitts30k} dataset and the St. Lucia \ref{fig:recalls:train_th:st_lucia} dataset.}
   \label{fig:recalls:train_th}
\end{figure}

\begin{figure}
   \centering
   \begin{subfigure}[b]{0.23\textwidth}
      \centering
      \includegraphics[width=\textwidth]{img/test_th/test_pitts30k_recalls_graph.png}
      \caption{Pitts30k}
      \label{fig:recalls:test_th:pitts30k}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{0.23\textwidth}
      \centering
      \includegraphics[width=\textwidth]{img/test_th/test_st_lucia_recalls_graph.png}
      \caption{St. Lucia}
      \label{fig:recalls:test_th:st_lucia}
   \end{subfigure}
   \caption{Graph showing the recalls obtained with different test positive distance threshold on both the pitts30k \ref{fig:recalls:test_th:pitts30k} dataset and the St. Lucia \ref{fig:recalls:test_th:st_lucia} dataset.}
   \label{fig:recalls:test_th}
\end{figure}

\section{Conclusions}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
