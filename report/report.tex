% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{***} % *** Enter the CVPR Paper ID here
\def\confName{x}
\def\confYear{2022}


\begin{document}
	
%%%%%%%%% TITLE - PLEASE UPDATE
\title{Visual Geo-Localization}

\author{Matteo Gambino\\
	s287572
	\and
	Michele Pierro\\
	s287846
	\and
	Fabio Grillo\\
	s287873
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
	In order to predict the location of a query image by retrieving annotated photographs with 
	similar descriptors needs an efficient and reliable generation of those descriptors. 
	In order to accomplish that objective, is fundamental that the network focuses on portion
	of the various images that contains useful information and at the same time ignore not 
	informative areas like the ones containing elements like cars or pedestrians. For that 
	reason attention layers are fundamental in the proposed network. In addition to that we 
	are comparing state of the art techniques for the visual geolocalization task like GeM \cite{GEM}, 
	NetVLAD \cite{NETVLAD} and CRN \cite{CRN}. The code used is publicly available 
	\href{https://github.com/matteo6198/project_visual_geolocalization}{here}
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
	For what concerns geolocalization and its purpose the best results have been achieved through
	deep learning algorithms that, thanks to the analysis of pixels of a given photograph, could retrieve 
	very accurated informations using convolutional neural networks (CNN). Of course image retrieval
	is not an easy task; starting from classification tasks, such as recognizing the elements of an image,
	it aims at extracting the most interesting features for each image from a large database; then for a query image
	it has to extract useful features from that too and then  find a correspondence between the features
	of the query photograph and the ones inside the database. 
	\newline An other relevant aspect on which it is important to keep attention on is the place recognition problem that
	is very challenging. In fact we must figure out that there are lots of conditions under which a picture can be taken:
	for example there may be different illuminations for a given place; so it is important to understand what is the appropriate 
	representation of a place that is rich enough to distinguish similarly looking places. The system built over
	visual place recognition must respect different constraints in terms of time spent and accuracy for retrieving GPS coordinates
	of a certain photograph. Thus, before starting, consider that each image should be taken
	from different points of view, with different lighting conditions, with dynamic objects and in different long-term conditions.

\section{Related works}
	Solving visual geolocalization issues leads to the resolution of other sub-problems.
	\newline Image retrieval consists of different steps such as features extraction, feature aggregation and similarity research.
	The methods on which each step has been approached are different: for feature extraction the most important ones are
	the scale invariant feature transform (SIFT), SURF which tries to keep the computation faster;
	then RootSIFT which finds better descriptor than SIFT and speed-up computational time.
	\newline Feature aggregation comes from natural language processing trying to count the occurences of many words in a vector ("bag of words");
	after that it has become a cluster problem where to assign each item of a given set to a peculiar set which represents 
	components with the same feature: the Vector of Locally Aggregated Descriptors (VLAD) is a way to accomplish to this task, in which
	is computed the distance between the feature and the center if its cluster for a more specific representation.
	Weights are also assigned to each feature in order that high-value weights represent more discriminative features.
	Furthermore NetVLAD \cite{NETVLAD} has been proposed as a layer plugged in CNNs.
	\newline For what concerns similarity research we can assume that the euclidean distance is the oldest method to find the most similar
	vector of features to a given one and it is practicable until dimensions don't gropw up. In this case it is better to use PCA technique
	to afford dimensionality reduction.
	\newline Visual place recognition could also be seen from another domain using 3D based methods to deal with it. 3D datasets are used to find
	the location of 2D query image thanks to the use of particular instrumentation that allows to reconstruct images from 2D to 3D but
	causing a loss in terms of computation time and storage needs. For this purpose those methous are usually used with 2D-based methods
	that try to filter a limited number of candidates from the dataset and then processing 3D methods.

\section{Methods}
Like \cite{GEM}, \cite{NETVLAD}, \cite{CRN} we have casted the problem of place recognition as the task of image
retrieval. We have implemented 3 different networks all based on the ResNet-18 \cite{resnet} backbone
without the fully connected layers and the last convolution layer. On the top of this backbone we 
have inserted 5 different heads (only one at a time can be selected), inspired by the works of \cite{GEM}, \cite{NETVLAD}, \cite{CRN}, in order
to generate the image descriptors. 

\subsection{Base Head}
This is the simplest head we have used and it's necessary in order to have a baseline to compare the other
results. After the last convolution layer of ResNet-18 we have normalized the feature map and used average 
pooling in order to generate the descriptors. This simple head tries to extract from the query the spatial
information by comparing the average value of the features in a given area and represent the traditional way 
to extract those descriptors.

\subsection{GeM head}
Following the work of \cite{GEM}, we have used a Generalized Mean approach in order to extract better 
descriptors for the query image. The generalized mean we are using is defined as:
\begin{equation}
	f_k = ({\frac{1}{X_k}} \sum_{x \in X_k} x^{p_k} ) ^ {\frac{1}{p_k}}
\end{equation}
where $X_k $ represent one of the normalized features map and $p_k $ is the pooling parameter. This 
pooling parameter is expressing how much is localized the zone of the image the network is focusing on.
The $p_k $ parameter, although it can be learned and inserted into back propagation, it has been fixed
and a single value is used for each activation map as suggested by \cite{GEM}. We have
inserted a fully connected layer that takes as input the pooled features in order to whiten the image
descriptors since it has been shown by \cite{GEM} that this approach is providing better results than
using other strategies like PCA.

\subsection{NetVLAD head}\label{sec:NETVLAD}
Inspired by the work presented in \cite{NETVLAD}, we have implemented also a NetVLAD head that solves in an 
elegant way the problem of computing Vectors of Locally Aggregated Descriptors (VLAD), as described in \cite{VLAD},
in CNN. This network, in order to compute those VLAD descriptors, is using two different parts. 
The first is called soft-assignment branch that is replacing the hard assignment of a descriptor to a single cluster
with a soft assignment of the descriptor to every cluster. This is performed using a soft-max operation on top of the output of a 1x1
convolution layer. This operation produces the probabilities $s_k$ that a given descriptor $x_i$ belongs to a cluster $k$ by:
\begin{equation}
	s_k(x_i) = {{e^{W_k^T x_i+b_k}}\over{\sum_{k'} e^{W_{k'}^T x_i+b_{k'}}}}
\end{equation} 
The second part, denominated VLAD core, is effectively computing the 
VLAD representation of the image given an image descriptor $x$, the cluster centers $c$ and the computed soft assignments
$s$ following the equation:
\begin{equation}
	V(j,k) = \sum_{i=1}^N s_k(x_i) (x_i(j) - c_k(j))
\end{equation} 
where $x_i(j)$ and $c_k(j)$ represent respectively the j-th dimension of the i-th descriptor and the k-th cluster center.
The obtained descriptors are then intra-normalized (using a column-wise L-2 norm), flattened into a vector and then a
final L-2 norm operation is applied.
For this network is fundamental to initialize the cluster centers in order to obtain good performances. This initialization
is preformed in a preliminary step using a small subset of the training data available and consists in computing the features 
representations, using the pre-trained ResNet-18 backbone, and then extracting the descriptors by randomly selecting some of the 
obtained features. Then, in order to obtain the cluster's centroids, k-means is used over the computed descriptors and the 
weights in the convolution layer for soft-assignment are initialized, to reproduce the results that would have been obtained
with VLAD described in \cite{VLAD}, using:
\begin{equation}
   W = \alpha ({\frac{c}{{||c||}_2}} \cdot d )
\end{equation}
where $c$ and $d$ are respectively the computed clusters centers and descriptors, $\alpha$ is instead selected to be large
in order to better mimic the traditional VLAD.

\subsection{CRN head}
Seen the results provided from the previous implemented heads and the success of the attention layers to make 
a network focus on relevant only parts of an image, we have decided to add a context-aware re-weighting layer to the NetVLAD
head, following the approach proposed by \cite{CRN}. This is perfectly integrated in the NetVLAD architecture and
it has the duty to produce a map that rescales the weights produced by the soft-assignment step of the NetVLAD layer.
This layer is composed by an initial average pooling sub-layer
that has the duty to reduce the dimensionality of the feature maps produced by the backbone. 
Differently from what specified in \cite{CRN}, it is not reducing the features maps to a fixed 
size but it is simply reducing by a half the dimensions of the features. In order to capture features at different
spatial resolutions, $3$ convolution filters (with kernel sizes respectively of $3$, $5$, $7$) are applied to the pooled features.
The  output of those filter is concatenated and an additional 1x1 convolution filter is used in order to accumulate the 
features produced. The resulting mask is then upsampled, in order to restore the original features map dimensionality,
by using a bilinear interpolator. This results into a mask that is used to re-weight the scores produced by the soft-assignment, specified
into the NetVLAD description, as showed in figure \ref{fig:CRN:ark}. This last operation is performed into the context modulation layer. This layer is 
performing the element-wise product of the mask and the soft-assignment scores. The output of the context modulation layer is then used 
in the standard NetVLAD core instead of the soft-assignment scores. So the final VLAD descriptors are produced by following:
\begin{equation}
	V(k) = \sum_{l \in R} m_l a_l^k (x_l - c_k)
\end{equation}
where for each spatial location $l$ of the feature map the residual of the locality $x_l$ from the cluster center $c_k$ 
is multiplied by both the soft-assignment scores $a_l^k$ and the re-weighting mask $m_l$. 
Also this head requires the initialization of centroids as the 
NetVLAD head and the initialization adopted is the same described in the section \ref{sec:NETVLAD}.

\begin{figure}
	\centering
	\includegraphics[width=0.3\textwidth]{img/CRN.png}
	\caption{The architecture of the CRN head.}
	\label{fig:CRN:ark}
\end{figure}

\subsection{CRN2 head}
In order to reduce the number of parameters to be learned by the CRN head, we have implemented a second version of this head,
called CRN2, that exploits the ideas of concatenating multiple 3x3 filters in order to obtain the same receptive field of a
bigger filter but using less parameters as suggested in \cite{VGG}. In particular we have replaced the 5x5 and the 7x7 filters showed in figure \ref{fig:CRN:ark}
with respectively 2 and 3 stacked 3x3 filters as showed in figure \ref{fig:CRN2:ark}. In addition to that, we have used dilated convolution
in order to remove the pooling and upsampling layers by generating the mask directly at the desired resolution inspired by the work 
proposed in \cite{atrous}. This, although requires 
more computation, will produce more accurate masks that may help the network to better focus on the relevant parts of the images.
Also this head requires the initialization of centroids as the CRN and the NetVLAD heads
and the initialization adopted is the same described in the section \ref{sec:NETVLAD}.

\begin{figure}
	\centering
	\includegraphics[width=0.3\textwidth]{img/CRN2.png}
	\caption{The architecture of the CRN2 head.}
	\label{fig:CRN2:ark}
\end{figure}

\section{Experiments}
In this section we evaluate the results obtained by the implemented heads compared to the base head.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/pitts_out.png}
		\caption{Pitts30k}
		\label{fig:datasets:pitts30k}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/st_lucia_out.png}
		\begin{minipage}{1cm}
			\vfill
		\end{minipage}
		\vspace{0.2cm}
		\caption{St. Lucia}
		\label{fig:datasets:st_lucia}
	\end{subfigure}
	\caption{The location of the images contained in both the pitts30k \ref{fig:datasets:pitts30k} dataset and the St. Lucia \ref{fig:datasets:st_lucia} dataset.}
	\label{fig:datasets}
\end{figure}

\paragraph{Datasets}
The experiments have been run on the pitts30k dataset \cite{NETVLAD} that is containing 3 different predefined splits, respectively for 
training, validation and testing, composed by $10k$ images each. Those are images taken in the city of Pittsburgh from the Google street
view images. In addition, also the St. Lucia dataset \cite{st_lucia} has been used only for testing the models trained on the pitts30k dataset.
The location of the images contained in both datasets can be seen in figure \ref{fig:datasets}. Those two datasets have a significant domain shift
since the images of Pittsburgh are mostly in a urban scenario with scenes mostly composed by skyscrapers while the images on the St. Lucia dataset 
are taken in a more natural scenario where the trees are much more present than buildings. We are using those dataset in order to assess how well
the proposed models generalize on different scenarios.

\paragraph{Mining procedure}\label{par:mining}
In order to train our models we need to generate triplets in the form $\{I_q, I^+, I^-\}$: for each query image $I_q$ we are looking for
positive examples $I^+$ and negative ones $I^-$. In order to do so, for each query image we retrieve the images into the database that are
in a range specified by the positive distance threshold parameter. We use 2 possibly different values for this threshold at train and test time.
Among all images in this range, we select as best positive the one that has descriptors more similar to the query image and we use this image
as $I^+$. All the images not in the positive distance threshold range are considered as negative samples. We select by default the 10 
images, from the negative set, that have the most similar descriptors as the query image. We call those images hard negatives and
we are using those instead of randomly picking from the negative set in order to make the task for the network more challenging and for obtaining  
a more robust model. Since the mining procedure described depends on the descriptors that vary over the training procedure, we are periodically recalculating the 
triplets within the epochs.

\paragraph{Loss function}
We have set the problem of visual geo-localization in approximating the location of a query image by retrieving the nearest 
database images in the descriptors space. For that reason, the objective of our training procedure is to make geographically
close images have a similar representation in descriptor space and instead make as far as possible the representation of geographically far images.
This is possible by exploiting contrastive learning and the loss function that we are using is a standard triplet loss defined by
\begin{equation}
   L(q, p, n) = \sum_i^N max( {|| q - p ||}_2 - {|| q - n_i||}_2 + m, 0)
\end{equation}
where $p$, $q$ and $n_i$ represent the descriptors extracted by the query image, the positive image and the negative ones (by default we extract
$10$ negatives examples for each query image as described in \nameref{par:mining} and the number of negatives samples is denoted by $N$).
The parameter $m$ is instead specifying a margin between the positive and negative descriptor representation of the images.
In fact, if a negative sample has a distance in the descriptors space higher than the margin $m$ the resulting loss will be $0$ and, instead,
if the distance between the positive and negative descriptors is lower than the margin the loss will be proportional to the margin violation.

\paragraph{Metric adopted}
Those results have been obtained by evaluating the models by using a standard evaluation procedure for place recognition.
A given query image is said to be correctly localized if at least one of the $N$ retrieved images is placed at a distance lower
or equal to test positive distance threshold from the ground truth position. This distance is set, if not differently specified, to $25$ meters. After
that we are calculating the percentage of correctly classified images for different values of $N$ (indicated with $R@N$).  

\subsection{Comparison among the proposed heads}
The results of comparison between the various proposed heads are reported in table \ref{tab:base_results}. Those results have 
been obtained on the pitts30k test set. As it's possible to notice the heads that are giving best results are the CRN and CRN2. That 
shows that adding attention is essential for improving the quality of generated descriptors. It's important to notice also how 
the results are influenced by the number of produced descriptors. In fact both the NetVLAD and the CRNs heads are generating much 
more descriptors with respect to the GeM and the base heads and this seems correlated to higher recalls. Since the CRNs and the NetVLAD
heads are outperforming the other ones, we will focus more on those 3 during the rest of this section.

\begin{table}
	\centering
	\begin{tabular}{|l|c|c|c|c|c|}
		\hline
		& Descs.&        $R@1$   &        $R@5$   &        $R@10$  &        $R@20$   \\ \hline
		Base     & 256   &         60.1   &         80.6   &          87.4   &          91.7   \\
		GeM      & 256   &         71.6   &         87.0   &          91.0   &          94.0   \\
		NetVLAD  & 16384 &         79.1   &         89.3   &          92.3   &          94.4   \\ \hline
		CRN*     & 16384 &         81.7   & \textbf{90.7}  &  \textbf{93.4}  &  \textbf{95.3}  \\
		CRN2     & 16384 &\textbf{81.8}   & \textbf{90.7}  &          93.2   &          95.2   \\ \hline
	\end{tabular}
	\caption{Results on the pitts30k test set obtained with the various heads compared with the base head. The number of generated descriptors 
		is also shown in the column Descs.}
	\label{tab:base_results}
\end{table}

The table \ref{tab:base_results:st_lucia} is reporting the results obtained on the St. Lucia dataset. As it's possible to notice, 
those results are much lower than the ones obtained on the pitts30k dataset and this is probably due to the significant domain shift between the datasets.
However it's important to notice how also in this case attention is providing best results especially for lower recall values while
simpler methods like GeM are providing the best performances at higher recall values. 
\begin{table}
	\centering
	\begin{tabular}{|l|c|c|c|c|c|}
		\hline
		& Descs.&        $R@1$   &        $R@5$   &        $R@10$  &        $R@20$   \\ \hline
		Base     & 256   &         26.8   &         45.4   &          55.1   &          63.9   \\
		GeM      & 256   &         41.7   &         62.2   &  \textbf{70.6}  &  \textbf{81.4}   \\
		NetVLAD  & 16384 &         44.8   &         59.4   &          66.7   &          75.3   \\ \hline
		CRN*     & 16384 & \textbf{48.7}  & \textbf{63.3}  &          69.7   &          75.9  \\
		CRN2     & 16384 &         47.6   &         62.6   &          69.2   &          75.3   \\ \hline
	\end{tabular}
	\caption{Results on the St. Lucia test set obtained with the various heads compared with the base head. The number of generated descriptors 
		is also shown in the column Descs.}
	\label{tab:base_results:st_lucia}
\end{table}

\subsection{CRN and CRN2 models}
As it's possible to notice from table \ref{tab:base_results}, the performances of the CRN and CRN2 networks are very close one to each other as expected
from the definition of the two networks. It's important to notice that the CRN2 network is using less parameters with respect to the CRN network. In fact,
the CRN2 network is using only 245K parameters compared to the 529k parameters used by the CRN network for the generation of the mask.
As shown in the table \ref{tab:time}, the time required to extract descriptors for a single image is higher for the CRN2 network with an increase of 32.3\% 
of the required execution time. This is due to the removal of the downsampling layer present in the original CRN implementation that implies the analysis of the image 
at full resolution and this 
has to be taken in consideration during the deploy phase where the descriptors for the query images have to be computed online. We have also noticed that an increase
of the time required to produce descriptors is correlated to better recalls values with the exception of the CRN2 head which is providing the same results as the CRN head
and also this has to be taken in consideration while deploying an application using those networks.

\begin{table}
   \centering
   \begin{tabular}{|l|c|c|c|}
      \hline
      Network   &  $R@5$&Params. (k) &  Time (ms)\\\hline
      Base      &  80.6 &  	 0.0  	 &  15.76\\
      GeM       &  87.0 &  	65.8  	 &  15.81\\
      NetVLAD   &  89.3 &  	16.3  	 &  20.64\\
      CRN       &  90.7 &  545.9  	 &  21.51\\
      CRN2      &  90.7 &  262.0  	 &  28.47\\\hline
   \end{tabular}
   \caption{The running time for computing the descriptors of a single image for the various networks computed on a GPU NVIDIA Tesla K80. The $R@5$ are computed on the pitts30k test set.
   The number of parameters are not including the common backbone and, if required by the network, are not considering the cluster centroids. }
   \label{tab:time}
\end{table}

\subsection{Visual results}
In this section we are going to show some visual results obtained with the various heads. In the figure \ref{fig:visual} we are 
reporting the results obtained with the NetVLAD, CRN and CRN2 heads on a query image taken from the pitts30k dataset. How it's 
possible to notice from the second column of the figure \ref{fig:visual} that shows the soft-assignment scores (summed over the 64 clusters used) heatmap 
on the query 
image, the context-aware re-weighting layers used are fundamental in order to make the network focus only on the relevant parts of the image.
It's important to notice that the CRN2 network is focusing at smaller portions of the query image and this can increase the difficulty in finding 
really discriminative zones of the query while the CRN model is focusing on wider areas that include the most relevant parts of the image.

\begin{figure}
   \centering
   \setlength{\tabcolsep}{-0.1pt}   
   \addtolength{\tabcolsep}{-0.1pt} 
   \def\arraystretch{0.2}
   \begin{tabular}{ccc}
      Query & Soft-assignment & Prediction\\
            & \includegraphics[width=0.15\textwidth]{img/results/29/29_NETVLAD_Cam_On_Image.png} & \includegraphics[width=0.15\textwidth]{img/results/29/29_NETVLAD_best_pred.png}\\
      \includegraphics[width=0.15\textwidth]{img/results/29/29_query.png} & \includegraphics[width=0.15\textwidth]{img/results/29/29_CRN_Cam_On_Image.png} &  \includegraphics[width=0.15\textwidth]{img/results/29/29_CRN_best_pred.png}\\
            & \includegraphics[width=0.15\textwidth]{img/results/29/29_CRN2_Cam_On_Image.png}    & \includegraphics[width=0.15\textwidth]{img/results/29/29_CRN2_best_pred.png}
   \end{tabular}
   \caption{Results on a image from the pitts30k dataset. The first row is showing results obtained with NetVLAD, the second with the CRN network, the third one with CRN2 network all on the same query image shown in the first column.
   The soft-assignment column is computed by summing over the 64 cluster soft-assignment scores. }
   \label{fig:visual}
\end{figure}
	
\section{Ablation study}
In this section we discuss the effect of changing one by one some paramethers of the NetVLAD network, we expecially focused on trying different learning rates, modifying the distance at which positives are taken.\\ We also changed the input images of the dataset by implementing some data augmentation tecniques and by changing the resolution of the images.\\
As optimizer algorithm we decided to use the Stochastic Gradient Descent (SGD), the results of the application of this iterative method, displayed in the table \ref{tab:SGD}, are an increment of the recall int the Pitts30k dataset and a decrement in the St, Lucia dataset.\\
pitts30k
\begin{table}[!h]
	\centering
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		.&        $R@1$   &        $R@5$   &        $R@10$  &        $R@20$   \\ \hline
		NetVLAD  Pitts30k     &         79.1   &         89.3   &          92.3   &          94.4   \\
		NetVLAD (SGD) Pitts30k  &         \textbf{79.5}   &         \textbf{90.4}   &          \textbf{93.0}   &          \textbf{95.0}   \\ \hline
		NetVLAD St. Lucia        &         \textbf{43.0}   &         \textbf{58.5}   &          \textbf{67.4}   &          \textbf{74.7}   \\
		NetVLAD (SGD) St. Lucia      &         42.1   & 56.8  &  63.7  &  71.4  \\
		 \hline
	\end{tabular}
	\caption{Results on the pitts30k and St. Lucia test sets obtained with the NetVLAD head compared with the NetVLAD+SDG head.}
	\label{tab:SGD}
\end{table}

\subsection{Comparison between different learning rates}
As first ablation study we tried different learning rates, from the table we can see that the best results in calculating the percentage of correctly classified images are obtained with a learning rate of 1 , with this learning rate the network is superior to the other configuration of learning rate in each case. We also noticed that by decreasing the learning rate we increment the number of epochs needed to end the training.
\begin{table}[!h]
	\centering
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		&          $R@1$  &        $R@5$  &        $R@10$ &        $R@20$   \\ \hline     
		lr = 1e-3 &         78.6    &    89.4       &    92.5       &         94.7       \\
		lr = 1e-4 &         60.0    &         79.7  & 86.4 & 91.2  \\    
		lr = 1e-5 & \textbf{82.3}   & \textbf{92.7} &         \textbf{95.0}  & \textbf{97.0}          \\
		\hline
	\end{tabular}
	\caption{Results obtained with the NetVLAD head on the pitts30k test set with different learning rates}
	\label{tab:NETVLAD:lr}
\end{table}
\subsection{Comparison between different positive distance treshold}
Initially the distance at which positive are taken was set at 25 meters, we tried to change the parameter ${val\_positives\_dist\_threshold}$ with different values. The graph contained in figure 6 shows that, during training, the positive distance threshold set at 5 meter outperforms all the other thresholds in every recall in both the Pitts30k and St Lucia datasets.\\
\begin{figure}[!h]
	\centering
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/train_th/test_pitts30k_recalls_graph.png}
		\caption{Pitts30k}
		\label{fig:recalls:train_th:pitts30k}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/train_th/test_st_lucia_recalls_graph.png}
		\caption{St. Lucia}
		\label{fig:recalls:train_th:st_lucia}
	\end{subfigure}
	\caption{Graph showing the recalls obtained with different train positive distance threshold on both the pitts30k \ref{fig:recalls:train_th:pitts30k} dataset and the St. Lucia \ref{fig:recalls:train_th:st_lucia} dataset.}
	\label{fig:recalls:train_th}
\end{figure}
We also tried different test positive distance threshold and in this case there is a big difference between the different distances, greater distances perfom in a better way than smaller ones.\\
By setting a larger positive distance we have worse performances because it leads to an increase in the false positives.
\begin{figure}[!h]
	\centering
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/test_th/test_pitts30k_recalls_graph.png}
		\caption{Pitts30k}
		\label{fig:recalls:test_th:pitts30k}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/test_th/test_st_lucia_recalls_graph.png}
		\caption{St. Lucia}
		\label{fig:recalls:test_th:st_lucia}
	\end{subfigure}
	\caption{Graph showing the recalls obtained with different test positive distance threshold on both the pitts30k \ref{fig:recalls:test_th:pitts30k} dataset and the St. Lucia \ref{fig:recalls:test_th:st_lucia} dataset.}
	\label{fig:recalls:test_th}
\end{figure}
In the Pitts30k dataset we can see that the recall on one image is 65.1 with the positive threshold distance set at 10 meters while the recall with the distance set at 50 meters is 82.9, there is a margin of 17.8\%\\
The same trend is mantained in all the different recalls on both the Pitts30k and St. Lucia datasets, but while in the Pitts30k the margin between the distance treshold set at 10 meters and the one set at 50 meters stabilizes, in the St Lucia Datasets keeps incrementing, initially it's 6\% at R1 and becomes 13\% at R20.

\subsection{Comparison between different data augmentation tecniques}
In order to see how the results change we tried some data augmentation techniques, we decided to use 3 of them: Color Jitter, Horizontal Flip + Rotate and Random Erasing. With Color Jitter we randomized the contrast and brightness of the image, with Horizontal Flip we flipped the image over the vertical axis and with the last trasformation we erase with random values the pixel contained inside a rectangle region.\\
\begin{figure}[!h]
	\centering
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/augment/test_pitts30k_recalls_graph.png}
		\caption{Pitts30k}
		\label{fig:recalls:augment:pitts30k}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/augment/test_st_lucia_recalls_graph.png}
		\caption{St. Lucia}
		\label{fig:recalls:augment:st_lucia}
	\end{subfigure}
	\caption{Graph showing the recalls obtained with different augmentation techniques on both the pitts30k \ref{fig:recalls:augment:pitts30k} dataset and the St. Lucia \ref{fig:recalls:augment:st_lucia} dataset.}
	\label{fig:recalls:augment}
\end{figure}
For the Pitts30k dataset we can see in figure \textcolor{red}{5a} that all the data augmentation techniques have a similar performance compared to the default configuration of the dataset; Only the flip and rotate technique gets sightly worse results in the first recalls.\\
For the St. Lucia dataset we can see in figure \textcolor{red}{5b} that all the data augmentation techniques have very different performance, in this case only the Color Jitter transformation gets better results than the default configurations, while  Random Erasing and Flip + Rotate get sightly worse results in all the recalls.\\
The Color Jitter tecnique is the only tecnique that gets better results because it's the only one that does not change the structure of the images.

\subsection{Comparison between different images sizes}
The last ablation study has been the variation of the images sizes by scaling their dimensions by a scaling factor. We decided to try 4 scaling factors (including the standard one 1.00), the value selected are: 0.50, 0.75, 1.00, and 1.25.\\
The results of the scaling factor are shown in figure \textcolor{red}{7}, for both the Pitts30k and St. Lucia dataset it's clear that a reduction of the image size leads to an increment in the recall's value; This increment is way more visible in the St. Lucia dataset (figure \textcolor{red}{7b}).
	\begin{figure}[!h]
	\centering
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/resize/test_pitts30k_recalls_graph.png}
		\caption{Pitts30k}
		\label{fig:recalls:resize:pitts30k}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/resize/test_st_lucia_recalls_graph.png}
		\caption{St. Lucia}
		\label{fig:recalls:resize:st_lucia}
	\end{subfigure}
	\caption{Graph showing the recalls obtained with different input image size on both the pitts30k \ref{fig:recalls:resize:pitts30k} dataset and the St. Lucia \ref{fig:recalls:resize:st_lucia} dataset.}
	\label{fig:recalls:resize}
\end{figure}

\section{Conclusions}

%%%%%%%%% REFERENCES
{\small
	\bibliographystyle{ieee_fullname}
	\bibliography{egbib}
}
	
\end{document}
